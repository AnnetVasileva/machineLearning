{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnetVasileva/machineLearning/blob/main/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtfC6uPs0waj"
      },
      "source": [
        "# import needed modules\r\n",
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSVU1XSX1ScS",
        "outputId": "f4a92741-8ea9-4875-be04-bd12f93d9f06"
      },
      "source": [
        "# we import the data from google drive, in case this is not the case for you please specify the directories yourself\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA7Dp3vh1gvG"
      },
      "source": [
        "# specifying directories for the files\r\n",
        "train_set = '/content/drive/MyDrive/ML_ECO/cup98lrn.csv'  # training set\r\n",
        "valid_set = '/content/drive/MyDrive/ML_ECO/cup98val.csv' # validation set\r\n",
        "val_target = '/content/drive/MyDrive/ML_ECO/valtarget.csv' # labels for the validation set"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFhW3ltc1ZDy"
      },
      "source": [
        "# read in data\n",
        "df = pd.read_csv(train_set,low_memory = False)  \n",
        "val = pd.read_csv(valid_set,low_memory = False)  #ASSUMING THE ORDER of columns is the same\n",
        "valtarget = pd.read_csv(val_target,low_memory = False)"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5yb3Dsga6uI"
      },
      "source": [
        "# make CONTROLN to an index in both datasets and then merge val and valtarget\n",
        "\n",
        "valtarget.index = valtarget.CONTROLN\n",
        "valtarget = valtarget.drop('CONTROLN', axis = 1)\n",
        "val.index = val.CONTROLN\n",
        "val = val.drop('CONTROLN', axis = 1)\n",
        "\n",
        "val = val.merge(valtarget, right_index= True, left_index=True)"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enUAV3Q9wduc"
      },
      "source": [
        "# next step is making sure that the order of the columns in train and validation data sets is the same\n",
        "df = df.drop('CONTROLN', axis = 1) # we do not need this anymore\n",
        "val = val[df.columns]"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Zh_fwcxATs"
      },
      "source": [
        "# now we append the rows from validation dataset to the df\n",
        "df = df.append(val)\n",
        "df.reset_index(inplace= True, drop = True)"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auoS8TtkwCr_"
      },
      "source": [
        "## Preprosessing\n",
        "Our preprosessing will include 4 steps:\n",
        "1. We will delete the columns we will not use (please see documentation to see which columns we drop)\n",
        "2. Re-code /transform complex variables to easier ones\n",
        "3. Encode ordinal variables using ordinal encoder and create one-hot-encoding where ordinal encoding is not suitable\n",
        "4. Delete observations with missing data or additionally drop features that contain more than 40% of missing data\n",
        "\n",
        "Please note: our dataset is highly imbalanced, TARGET_B, having values 1(donor) and 0 (not a donor) has a proportion of 5%/95%. This means that by justpredicting that everone is not a donor we already get an accuracy of 95%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMD82WM5fwr3"
      },
      "source": [
        "**STEP 1: Drop Features**\n",
        "\n",
        "In dropping features we follow certain rules, namely:\n",
        "1. We drop every feature that is a date\n",
        "2. We drop features that refer to the sources of information \n",
        "3. We drop features that are already covered implicitly by the other features that we will encode \n",
        "4. We drop features that have a very complex structure and would require a lot of dummy variables (like STATE)\n",
        "5. We drop features that highly depend on features that we previously decided to drop (like WEALTH2)\n",
        "6. We drop variables too specific and try to keep those more general to learn an overall relationship.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7jT4Bi2wk2z"
      },
      "source": [
        "# here we define some groups of variable\n",
        "# for more info please consult the cup98dic.txt\n",
        "\n",
        "yesno_variables = ['COLLECT1', 'VETERANS', 'BIBLE', 'CATLG', 'HOMEE', 'PETS', 'CDPLAY',\n",
        "       'STEREO', 'PCOWNERS', 'PHOTO', 'CRAFTS', 'FISHER', 'GARDENIN', 'BOATS',\n",
        "       'WALKER', 'KIDSTUFF', 'CARDS', 'PLATES']\n",
        "rfas = ['RFA_2','RFA_3', 'RFA_4', 'RFA_5', 'RFA_6', 'RFA_7', 'RFA_8', 'RFA_9', 'RFA_10',\n",
        "       'RFA_11', 'RFA_12', 'RFA_13', 'RFA_14', 'RFA_15', 'RFA_16', 'RFA_17',\n",
        "       'RFA_18', 'RFA_19', 'RFA_20', 'RFA_21', 'RFA_22', 'RFA_23', 'RFA_24'] # RFA variables - decided to drop and keep onl the most recent one\n",
        "\n",
        "drop = ['ODATEDW','OSOURCE','TCODE','STATE','ZIP','MAILCODE','PVASTATE','DOB','NOEXCH','RECINHSE','RECP3','RECPGVG','RECSWEEP','MDMAUD','WEALTH2','GEOCODE',\n",
        " 'ADATE_2','ADATE_3','ADATE_4','ADATE_5','ADATE_6','ADATE_7','ADATE_8','ADATE_9','ADATE_10','ADATE_11','ADATE_12','ADATE_13',\n",
        " 'ADATE_14','ADATE_15','ADATE_16','ADATE_17','ADATE_18','ADATE_19','ADATE_20','ADATE_21','ADATE_22','ADATE_23','ADATE_24',\n",
        " 'RDATE_3','RDATE_4','RDATE_5','RDATE_6','RDATE_7', 'RDATE_8', 'RDATE_9','RDATE_10', 'RDATE_11', 'RDATE_12', 'RDATE_13', 'RDATE_14', 'RDATE_15',\n",
        " 'RDATE_16','RDATE_17', 'RDATE_18', 'RDATE_19','RDATE_20', 'RDATE_21', 'RDATE_22', 'RDATE_23', 'RDATE_24', 'RAMNT_3', 'RAMNT_4', 'RAMNT_5', 'RAMNT_6',\n",
        " 'RAMNT_7', 'RAMNT_8', 'RAMNT_9', 'RAMNT_10', 'RAMNT_11', 'RAMNT_12', 'RAMNT_13', 'RAMNT_14', 'RAMNT_15', 'RAMNT_17', 'RAMNT_18', 'RAMNT_19', 'RAMNT_20',\n",
        " 'RAMNT_21', 'RAMNT_22', 'RAMNT_23', 'RAMNT_24', 'GEOCODE2', 'CLUSTER2', 'SOLP3', 'SOLIH', 'LASTDATE', 'FISTDATE', 'NEXTDATE', 'MALEMILI', 'MALEVET', 'VIETVETS',\n",
        " 'WWIIVETS', 'LOCALGOV', 'STATEGOV', 'FEDGOV', 'MSA', 'ADI', 'DMA','CLUSTER', 'AGEFLAG', 'DATASRCE', 'LIFESRC', 'MINRDATE', 'MAXRDATE', 'MAXADATE', 'NUMCHLD',\n",
        " 'ETH1', 'ETH2', 'ETH3','ETH4', 'ETH5', 'ETH6', 'ETH7', 'ETH8', 'ETH9', 'ETH10', 'ETH11',\n",
        "  'ETH12', 'ETH13', 'ETH14', 'ETH15', 'ETH16', \n",
        "  'AGE902','AGE903', 'AGE904', 'AGE905', 'AGE906', 'AGE907', 'CHIL1', 'CHIL2',\n",
        "  'CHIL3', 'AGEC1', 'AGEC2', 'AGEC3', 'AGEC4', 'AGEC5', 'AGEC6', 'AGEC7','CHILC1', 'CHILC2', 'CHILC3', \n",
        "  'CHILC5', 'HHAGE1', 'HHAGE2', 'HHAGE3', 'HHN1', 'HHN2', 'HHN3', 'HHN4','HHN5', 'HHN6', 'MARR1', 'MARR2', 'MARR3', 'MARR4', \n",
        "  'DW1', 'DW2', 'DW3', 'DW4', 'DW5', 'DW6', 'DW7', 'DW8', 'DW9',\n",
        "  'HU2', 'HU3', 'HU4', 'HU5', 'HHD1', 'HHD2', 'HHD3', 'HHD4', 'HHD5',\n",
        "  'HHD6', 'HHD7', 'HHD8', 'HHD9', 'HHD10', 'HHD11', 'HHD12', \n",
        "  'HUR1', 'HUR2', 'RHP1', 'RHP2', 'RHP3', 'RHP4', 'HUPA1', 'HUPA2',\n",
        "  'HUPA3', 'HUPA4', 'HUPA5', 'HUPA6', 'HUPA7', 'RP1', 'RP2', 'RP3', 'RP4',\n",
        "  'IC15', 'IC16', 'IC17', 'IC18', 'IC19', 'IC20', 'IC21', 'IC22', 'IC23', \n",
        "  'EIC1', 'EIC2', 'EIC3', 'EIC4', 'EIC5', 'EIC6', 'EIC7', 'EIC8', 'EIC9',\n",
        "  'EIC10', 'EIC11', 'EIC12', 'EIC13', 'EIC14', 'EIC15', 'EIC16', 'OEDC1','OEDC2', 'OEDC3', 'OEDC4', 'OEDC5', 'OEDC6', 'OEDC7',\n",
        "  'EC3', 'EC4', 'EC5', 'EC6', 'EC7', 'EC8', 'SEC1', 'SEC2', 'SEC3',\n",
        "       'SEC4', 'SEC5', 'AFC1', 'AFC2', 'AFC3', 'AFC4', 'AFC5', 'AFC6', 'VC1',\n",
        "       'VC2', 'VC3', 'VC4', 'ANC1', 'ANC2', 'ANC3', 'ANC4', 'ANC5', 'ANC6',\n",
        "       'ANC7', 'ANC8', 'ANC9', 'ANC10', 'ANC11', 'ANC12', 'ANC13', 'ANC14','ANC15',\n",
        "        'HC3', 'HC4', 'HC5', 'HC6', 'HC7', 'HC8', 'HC9', 'HC10', 'HC11', 'HC12',\n",
        "       'HC13', 'HC14', 'HC15', 'HC16', 'HC17', 'HC18', 'HC19', 'HC20', 'HC21']"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM4ZfXNGyemS"
      },
      "source": [
        "df = df.drop(drop, axis = 1)\n",
        "df = df.drop(rfas, axis = 1)"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOFjVokF1_SG"
      },
      "source": [
        "**STEP 2 : Encode**\n",
        "\n",
        "Now we start encoding things\n",
        "1. We first rename the first column to Index\n",
        "2. We then extract the first letter from DOMAIN to then encode it to one-hot encoding, the second letter will correspond to the Social Status (SES)\n",
        "3. Afterwards we change the CHILDXX variables , which will be also encoded as one-hot \n",
        "4. We will then encode yes/no variables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96niBL_Axrty"
      },
      "source": [
        "df = df.rename(columns = {'Unnamed: 0' : 'Index'}) # rename first column to the Index"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qSvlAB7ljre",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8904eb83-6139-4650-9d31-2d668e268d00"
      },
      "source": [
        "df.DOMAIN = df.DOMAIN.replace({' ': '99'}) # replace empty cells to 99 and then every 9 will be replaced to NaN\n",
        "\n",
        "df['SES'] = 0 # NEW variable - socioeconomic status - second byte from DOMAIN\n",
        "index = 0\n",
        "for x in df.DOMAIN:\n",
        "    df.SES[index] = int(df.DOMAIN[index][1])\n",
        "    index += 1\n",
        "\n",
        "\n",
        "# extracting the first byte\n",
        "index = 0\n",
        "for x in df.DOMAIN:\n",
        "    df.DOMAIN[index] = df.DOMAIN[index][0]\n",
        "    index += 1\n"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyu-c3Wpq4Cc"
      },
      "source": [
        "df.HOMEOWNR = df.HOMEOWNR.replace({'H': 1, 'U': 0, ' ': np.NaN})\n",
        "df['CHILD03'] = df['CHILD03'].replace({' ': 1, 'B': 2, 'F': 3, 'M':4}) # we will assume that no indication of \n",
        "# children means no children as a category\n",
        "df['CHILD07'] = df['CHILD07'].replace({' ': 1, 'B': 2, 'F': 3, 'M':4})\t\n",
        "df['CHILD12'] = df['CHILD12'].replace({' ': 1, 'B': 2, 'F': 3, 'M':4})\n",
        "df['CHILD18'] = df['CHILD18'].replace({' ': 1, 'B': 2, 'F': 3, 'M':4})#\n",
        "df.PEPSTRFL = df.PEPSTRFL.replace({'X': 1, ' ':0})\n",
        "df.DOMAIN = df.DOMAIN.replace({9 : np.NaN, '9': np.NaN})\n",
        "df.DOMAIN = df.DOMAIN.replace({'S': 1, 'T':2, 'R':3, 'C':4, 'U':5})\n",
        "df.SES = df.SES.replace({9 : np.NaN, 4: 3}) # please read documentation, the decision was made to replace every 4 to 3 so that technically 3 will include \n",
        "# all lowest SES\n",
        "df.SES = df.SES.replace({3: 1, 1: 3}) # now we replace all 3 with 1 and all 1 with 3 so that we can decode it ordinally\n",
        "df.GENDER = df.GENDER.replace({'A': 'U', 'C': 'U', ' ': np.NaN, 'J' : 'U'})\n",
        "df.MDMAUD_F = df.MDMAUD_F.replace({'X': np.NaN})\n",
        "\n",
        "\n",
        "for item in yesno_variables:\n",
        "    df[item] = df[item].replace({' ': 0, 'N':0, 'Y': 1}) # these data values are a bit ambiguous since some of them do not have real N so we assume\n",
        "    #that empty cells represent negative observations\n"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kmYN9A0GaKc"
      },
      "source": [
        "**STEP 3: Delete values with nans**\n",
        "\n",
        "We further get rid of the features that have more than 40% of missing values\n",
        "We further drop the index since we wont need it\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCXbM8eDuchI"
      },
      "source": [
        "too_many_nas = [] # columns where we still have too many nans\n",
        "\n",
        "for item in df:\n",
        "    if df[item].isna().sum() > len(df)*0.4: # more tan 40% of all values is the threshold\n",
        "      too_many_nas.append(str(item))\n",
        "\n",
        "df = df.drop(too_many_nas, axis = 1)\n",
        "df = df.drop('Index', axis = 1)"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZHSy-O1rFgm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a3190c1-7e9a-41ca-e18f-d51337c1222d"
      },
      "source": [
        "print(df.shape)"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(191779, 156)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gsKo9RgWXa6"
      },
      "source": [
        "**STEP 4: One Hot Encoding and Categorical Encoding**\n",
        "\n",
        "We first drop rows with nans and then specify features to encode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MYZUFWWWbS6"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "#dropna\n",
        "df = df.dropna()\n",
        "\n",
        "one_hot_encoding = ['DOMAIN', 'CHILD03','CHILD12','CHILD18', 'GENDER', 'MDMAUD_R', 'RFA_2R', 'MAJOR'] \n",
        "categorical_encoding = ['MDMAUD_A', 'SES', 'INCOME','RFA_2F', 'RFA_2A']\n",
        "categories = [[ 'X' ,'L', 'C', 'M', 'T'],\n",
        "              ['1','2','3'],\n",
        "              [1.,2.,3.,4.,5.,6.,7.],\n",
        "              [1.0,2.0,3.0,4.0],\n",
        "              ['A', 'B', 'C', 'D', 'E', 'F', 'G']]\n",
        "\n",
        "\n",
        "ordinal_encoder = OrdinalEncoder(categories = categories)\n",
        "one_hot_encoder = OneHotEncoder()\n",
        "\n",
        "df_onehot = one_hot_encoder.fit_transform(df[one_hot_encoding])\n",
        "df_ordinal = ordinal_encoder.fit_transform(df[categorical_encoding])\n"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMx7_chKrdjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe455309-a34e-468c-e9f4-158748168695"
      },
      "source": [
        "print(df.shape)"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(111680, 156)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgbeFV8lgc5J"
      },
      "source": [
        "# TARGET VARIABLES\n",
        "label_binary = df['TARGET_B'].to_numpy() # 1/0 donated or not\n",
        "label_dollars = df['TARGET_D'].to_numpy() # how much donated\n",
        "\n",
        "\n",
        "columns = np.setdiff1d(df.columns,one_hot_encoding)\n",
        "columns = np.setdiff1d(columns,categorical_encoding)\n",
        "columns = np.setdiff1d(columns,['TARGET_B','TARGET_D'])\n",
        "\n",
        "df = df[columns]"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN-hEKOUrhsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85968fe1-e724-4e65-8fb2-eb2ebed93418"
      },
      "source": [
        "print(df.shape)"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(111680, 141)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0e3zWm8h6Pr"
      },
      "source": [
        "df = np.concatenate((df,df_onehot.toarray(), df_ordinal), axis = 1)"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZpR8ghVrmxL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "337f9819-0a53-4730-d4c9-36ee1e78a956"
      },
      "source": [
        "print(df.shape)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(111680, 174)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejAnC9zhYao-"
      },
      "source": [
        "# FINAL RESULT SHOULD BE df and a label\n",
        "import pickle\n",
        "data = {}\n",
        "data['X'] = df\n",
        "data['y'] = label_binary\n",
        "pickle_path = '/content/drive/MyDrive/ML_ECO/data_set.pkl'\n",
        "with open(pickle_path, \"wb\") as f:\n",
        "    pickle.dump(data, f)"
      ],
      "execution_count": 217,
      "outputs": []
    }
  ]
}
